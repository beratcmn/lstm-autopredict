{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "from collections import Counter\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path, seq_length, min_word_freq=2):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        self.words = self.tokenize(text)\n",
    "        word_counts = Counter(self.words)\n",
    "        \n",
    "        # Create vocabulary with words appearing at least min_word_freq times\n",
    "        self.word_to_ix = {word: i for i, (word, count) in enumerate(word_counts.items()) if count >= min_word_freq}\n",
    "        self.word_to_ix['<UNK>'] = len(self.word_to_ix)  # Add unknown token\n",
    "        self.ix_to_word = {i: word for word, i in self.word_to_ix.items()}\n",
    "        \n",
    "        self.data = [self.word_to_ix.get(w, self.word_to_ix['<UNK>']) for w in self.words]\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # Debug: Print vocabulary size and a few samples\n",
    "        print(f\"Vocabulary size: {len(self.word_to_ix)}\")\n",
    "        print(f\"Sample data indices: {self.data[:10]}\")\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.data[index:index+self.seq_length]),\n",
    "            torch.tensor(self.data[index+1:index+self.seq_length+1])\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, num_epochs, learning_rate, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_inputs, batch_targets in dataloader:\n",
    "            batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_inputs)\n",
    "            \n",
    "            # Debugging: Print the shape of outputs and batch_targets\n",
    "            # print(f\"Outputs shape: {outputs.shape}\")\n",
    "            # print(f\"Batch targets shape: {batch_targets.shape}\")\n",
    "            \n",
    "            loss = criterion(outputs.view(-1, len(dataset.word_to_ix)), batch_targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_autocomplete(model, input_text, word_to_ix, ix_to_word, device, num_suggestions=3):\n",
    "    model.eval()\n",
    "    words = input_text.lower().split()\n",
    "    input_seq = [word_to_ix.get(word, word_to_ix['<UNK>']) for word in words]\n",
    "    input_tensor = torch.tensor(input_seq).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = torch.softmax(output[0, -1], dim=0)\n",
    "        top_indices = torch.topk(probabilities, num_suggestions).indices.tolist()\n",
    "    \n",
    "    suggestions = [ix_to_word[idx] for idx in top_indices]\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"data_extended.txt\"\n",
    "SEQ_LENGTH = 5\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "MIN_WORD_FREQ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Vocabulary size: 1065\n",
      "Sample data indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Vocabulary size: 1065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "LanguageModel                            [32, 5, 1065]             --\n",
       "├─Embedding: 1-1                         [32, 5, 100]              106,500\n",
       "├─LSTM: 1-2                              [32, 5, 256]              366,592\n",
       "├─Linear: 1-3                            [32, 5, 1065]             273,705\n",
       "==========================================================================================\n",
       "Total params: 746,797\n",
       "Trainable params: 746,797\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 70.82\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 1.82\n",
       "Params size (MB): 2.99\n",
       "Estimated Total Size (MB): 4.81\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "dataset = TextDataset(FILE_PATH, SEQ_LENGTH, MIN_WORD_FREQ)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "vocab_size = len(dataset.word_to_ix)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "model = LanguageModel(vocab_size, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "summary(model, input_size=(BATCH_SIZE, SEQ_LENGTH), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 6.4788\n",
      "Epoch 2/100, Loss: 4.8638\n",
      "Epoch 3/100, Loss: 3.1513\n",
      "Epoch 4/100, Loss: 1.9733\n",
      "Epoch 5/100, Loss: 1.3504\n",
      "Epoch 6/100, Loss: 1.0215\n",
      "Epoch 7/100, Loss: 0.8272\n",
      "Epoch 8/100, Loss: 0.6981\n",
      "Epoch 9/100, Loss: 0.6071\n",
      "Epoch 10/100, Loss: 0.5402\n",
      "Epoch 11/100, Loss: 0.4902\n",
      "Epoch 12/100, Loss: 0.4566\n",
      "Epoch 13/100, Loss: 0.4311\n",
      "Epoch 14/100, Loss: 0.4118\n",
      "Epoch 15/100, Loss: 0.3984\n",
      "Epoch 16/100, Loss: 0.3880\n",
      "Epoch 17/100, Loss: 0.3814\n",
      "Epoch 18/100, Loss: 0.3754\n",
      "Epoch 19/100, Loss: 0.3719\n",
      "Epoch 20/100, Loss: 0.3653\n",
      "Epoch 21/100, Loss: 0.3619\n",
      "Epoch 22/100, Loss: 0.3603\n",
      "Epoch 23/100, Loss: 0.3578\n",
      "Epoch 24/100, Loss: 0.3584\n",
      "Epoch 25/100, Loss: 0.3557\n",
      "Epoch 26/100, Loss: 0.3525\n",
      "Epoch 27/100, Loss: 0.3510\n",
      "Epoch 28/100, Loss: 0.3503\n",
      "Epoch 29/100, Loss: 0.3502\n",
      "Epoch 30/100, Loss: 0.3512\n",
      "Epoch 31/100, Loss: 0.3493\n",
      "Epoch 32/100, Loss: 0.3482\n",
      "Epoch 33/100, Loss: 0.3468\n",
      "Epoch 34/100, Loss: 0.3463\n",
      "Epoch 35/100, Loss: 0.3470\n",
      "Epoch 36/100, Loss: 0.3455\n",
      "Epoch 37/100, Loss: 0.3454\n",
      "Epoch 38/100, Loss: 0.3437\n",
      "Epoch 39/100, Loss: 0.3451\n",
      "Epoch 40/100, Loss: 0.3438\n",
      "Epoch 41/100, Loss: 0.3431\n",
      "Epoch 42/100, Loss: 0.3431\n",
      "Epoch 43/100, Loss: 0.3419\n",
      "Epoch 44/100, Loss: 0.3405\n",
      "Epoch 45/100, Loss: 0.3427\n",
      "Epoch 46/100, Loss: 0.3415\n",
      "Epoch 47/100, Loss: 0.3390\n",
      "Epoch 48/100, Loss: 0.3401\n",
      "Epoch 49/100, Loss: 0.3415\n",
      "Epoch 50/100, Loss: 0.3383\n",
      "Epoch 51/100, Loss: 0.3397\n",
      "Epoch 52/100, Loss: 0.3393\n",
      "Epoch 53/100, Loss: 0.3386\n",
      "Epoch 54/100, Loss: 0.3374\n",
      "Epoch 55/100, Loss: 0.3359\n",
      "Epoch 56/100, Loss: 0.3382\n",
      "Epoch 57/100, Loss: 0.3361\n",
      "Epoch 58/100, Loss: 0.3373\n",
      "Epoch 59/100, Loss: 0.3374\n",
      "Epoch 60/100, Loss: 0.3371\n",
      "Epoch 61/100, Loss: 0.3356\n",
      "Epoch 62/100, Loss: 0.3365\n",
      "Epoch 63/100, Loss: 0.3358\n",
      "Epoch 64/100, Loss: 0.3357\n",
      "Epoch 65/100, Loss: 0.3350\n",
      "Epoch 66/100, Loss: 0.3345\n",
      "Epoch 67/100, Loss: 0.3346\n",
      "Epoch 68/100, Loss: 0.3321\n",
      "Epoch 69/100, Loss: 0.3341\n",
      "Epoch 70/100, Loss: 0.3337\n",
      "Epoch 71/100, Loss: 0.3346\n",
      "Epoch 72/100, Loss: 0.3342\n",
      "Epoch 73/100, Loss: 0.3343\n",
      "Epoch 74/100, Loss: 0.3318\n",
      "Epoch 75/100, Loss: 0.3337\n",
      "Epoch 76/100, Loss: 0.3326\n",
      "Epoch 77/100, Loss: 0.3329\n",
      "Epoch 78/100, Loss: 0.3327\n",
      "Epoch 79/100, Loss: 0.3336\n",
      "Epoch 80/100, Loss: 0.3315\n",
      "Epoch 81/100, Loss: 0.3326\n",
      "Epoch 82/100, Loss: 0.3297\n",
      "Epoch 83/100, Loss: 0.3309\n",
      "Epoch 84/100, Loss: 0.3314\n",
      "Epoch 85/100, Loss: 0.3299\n",
      "Epoch 86/100, Loss: 0.3303\n",
      "Epoch 87/100, Loss: 0.3295\n",
      "Epoch 88/100, Loss: 0.3319\n",
      "Epoch 89/100, Loss: 0.3306\n",
      "Epoch 90/100, Loss: 0.3284\n",
      "Epoch 91/100, Loss: 0.3300\n",
      "Epoch 92/100, Loss: 0.3307\n",
      "Epoch 93/100, Loss: 0.3298\n",
      "Epoch 94/100, Loss: 0.3287\n",
      "Epoch 95/100, Loss: 0.3296\n",
      "Epoch 96/100, Loss: 0.3295\n",
      "Epoch 97/100, Loss: 0.3287\n",
      "Epoch 98/100, Loss: 0.3280\n",
      "Epoch 99/100, Loss: 0.3288\n",
      "Epoch 100/100, Loss: 0.3282\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, dataloader, NUM_EPOCHS, LEARNING_RATE, device)\n",
    "torch.save(trained_model.state_dict(), \"autocomplete_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: nasıl\n",
      "Suggestions: gidebilirim, bir, yardımcı\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_inputs = [\n",
    "    \"nasıl\",\n",
    "]\n",
    "\n",
    "trained_model.to(device)\n",
    "for input_text in test_inputs:\n",
    "    suggestions = generate_autocomplete(trained_model, input_text, dataset.word_to_ix, dataset.ix_to_word, device)\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Suggestions: {', '.join(suggestions)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
